{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1272091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils import data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.backends import cudnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3276aa",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6475b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDPAttention(nn.Module):\n",
    "    def __init__(self, dim_key, dim_val, masked=False):\n",
    "        super().__init__()\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_val = dim_val\n",
    "        self.masked = masked\n",
    "        self.scale = math.sqrt(self.dim_val)\n",
    "        self.sm = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        B, L, D = q.size()\n",
    "        output = torch.matmul(q, torch.transpose(k, 1, 2))\n",
    "        output = torch.div(output, self.scale)\n",
    "        if self.masked:\n",
    "            mask = (-1*torch.ones(L,L)*float('inf')).triu(1)\n",
    "            output += mask\n",
    "#         output = output + q.size()[1]\n",
    "        output = self.sm(output)\n",
    "        output = torch.matmul(output, v)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, h, masked=False): # dim_key, dim_val = dim_model/h (?)\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_val = dim_val\n",
    "        self.dim_head = dim_model // h\n",
    "        \n",
    "        self.v_layers = []\n",
    "        self.k_layers = []\n",
    "        self.q_layers = []\n",
    "        self.attention_layers = []\n",
    "        for i in range(self.h):\n",
    "            self.q_layers.append(nn.Linear(in_features=dim_model, out_features=self.dim_head, bias=False))\n",
    "            self.k_layers.append(nn.Linear(in_features=dim_model, out_features=self.dim_head, bias=False))\n",
    "            self.v_layers.append(nn.Linear(in_features=dim_model, out_features=self.dim_head, bias=False))\n",
    "            self.attention_layers.append(ScaledDPAttention(self.dim_head, self.dim_head, masked))\n",
    "        self.linear = nn.Linear(in_features = h*self.dim_head, out_features=dim_model, bias=False)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        outs = []\n",
    "        for i in range(self.h):\n",
    "            q = self.q_layers[i](Q)\n",
    "            k = self.k_layers[i](K)\n",
    "            v = self.v_layers[i](V)\n",
    "            o = self.attention_layers[i](q, k, v)\n",
    "            outs.append(o)\n",
    "        output = torch.cat(outs, dim=2) # check dimenson \n",
    "        return self.linear(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645a518",
   "metadata": {},
   "source": [
    "### Debug \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fe7f7fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 64])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_model = 64\n",
    "dim_key = 8\n",
    "dim_val = 8\n",
    "K = torch.ones(10,16,dim_model)\n",
    "V = torch.ones(10,16,dim_model)\n",
    "Q = torch.ones(10,16,dim_model)\n",
    "sdp_attention = ScaledDPAttention(dim_key, dim_val, masked=False)\n",
    "output = sdp_attention(K, V, Q)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d39f4c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 64])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sdp_attention = ScaledDPAttention(dim_key, dim_val, masked=True)\n",
    "output = masked_sdp_attention(K, V, Q)\n",
    "output.shape\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "097ffd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 64])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_model = 64\n",
    "h = 8\n",
    "mh_attentaion = MultiHeadAttention(dim_model, dim_key, dim_val, h)\n",
    "output = mh_attentaion(Q, K, V)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147877b1",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48378401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(batch_size, n_dim, embedding_length):\n",
    "    get_pos = lambda pos : [pos/(10000**(2*(i//2)/n_dim)) for i in range(n_dim)]\n",
    "    code = np.array([get_pos(i) for i in range(embedding_length)])\n",
    "    encoding = np.zeros((embedding_length, n_dim))\n",
    "    encoding[:, 0::2] = np.sin(code[:, 0::2])\n",
    "    encoding[:, 1::2] = np.cos(code[:, 1::2])\n",
    "    return np.tile(encoding, (batch_size, 1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670d950",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "776b9126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 125, 125)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim = 125\n",
    "batch_size = 8\n",
    "embedding_length = 125\n",
    "pe = positional_encoding(batch_size, n_dim, embedding_length)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109bf2d8",
   "metadata": {},
   "source": [
    "## Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "efcfa708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, dim_hidden, h=8):\n",
    "        super().__init__()        \n",
    "        self.attention = MultiHeadAttention(dim_model, dim_key, dim_val, h)\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(in_features=dim_model, out_features=dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=dim_hidden, out_features=dim_model)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        A = self.attention(Q = X, K = X, V = X)\n",
    "        A = self.norm(A + X)\n",
    "        F = self.FFN(A)\n",
    "        return self.norm(F + A)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5c8f2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, dim_hidden, h=8):\n",
    "        super().__init__()\n",
    "        self.masked_attention = MultiHeadAttention(dim_model, dim_key, dim_val, h, masked=True)\n",
    "        self.attention = MultiHeadAttention(dim_model, dim_key, dim_val, h, masked=False)\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(in_features=dim_model, out_features=dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=dim_hidden, out_features=dim_model)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "    \n",
    "    def forward(self, inputs): # (X, encoder_feature)\n",
    "        X, features = inputs\n",
    "        masked_A = self.masked_attention(Q = X, K = X, V = X)\n",
    "        masked_A = self.norm(masked_A + X)\n",
    "        A = self.attention(Q = masked_A, K = features, V = features)\n",
    "        A = self.norm(A + masked_A)\n",
    "        F = self.FFN(A)\n",
    "        F = self.norm(F + A)\n",
    "        return (F, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6dfbb3",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ce220a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 125, 125])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_hidden=64\n",
    "dim_key = n_dim\n",
    "dim_val = n_dim\n",
    "dim_model = n_dim\n",
    "X = torch.zeros(batch_size, embedding_length, n_dim)\n",
    "encoder_layer = EncoderLayer(dim_model, dim_key, dim_val, dim_hidden, h)\n",
    "encoder_output = encoder_layer(X)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1e357843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 125, 125])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 125, 125])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_layer = DecoderLayer(dim_model, dim_key, dim_val, dim_hidden, h)\n",
    "decoder_input = (X, encoder_output)\n",
    "output = decoder_layer(decoder_input)\n",
    "display(output[0].shape)\n",
    "display(output[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a3e79",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2147c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, dim_hidden, output_dim, N=6, h=8):\n",
    "        super().__init__()\n",
    "        encoder_layers = []\n",
    "        decoder_layers = []\n",
    "\n",
    "        for i in range(N):\n",
    "            encoder_layers.append(EncoderLayer(dim_model, dim_key, dim_val, dim_hidden, h))\n",
    "            decoder_layers.append(DecoderLayer(dim_model, dim_key, dim_val, dim_hidden, h)) \n",
    "\n",
    "        self.Encoder = nn.Sequential(*encoder_layers)\n",
    "        self.Decoder = nn.Sequential(*decoder_layers)        \n",
    "        self.FC = nn.Linear(in_features=dim_model, out_features=output_dim) # weights? bias?\n",
    "        self.sm = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, inputs, outputs):\n",
    "        features = self.Encoder(inputs)\n",
    "        d, _ = self.Decoder((outputs, features))\n",
    "        return self.sm(self.FC(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43446bdc",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "40f99f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 125, 1000])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = 1000 # dictionary size\n",
    "model = Transformer(dim_model, dim_key, dim_val, dim_hidden, output_dim)\n",
    "model_out = model(X, X)\n",
    "model_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e365b",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f6b79",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
