{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1272091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils import data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.backends import cudnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6475b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDPAttention(nn.Module):\n",
    "    def __init__(self, dim_key, dim_val, masked=False):\n",
    "        super().__init__()\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_val = dim_val\n",
    "        self.masked = masked\n",
    "        self.scale = math.sqrt(self.dim_val)\n",
    "    \n",
    "    def forward(q, k, v):\n",
    "        B, L, D = q.size()\n",
    "        output = torch.matmul(q, k)\n",
    "        output = torch.div(output, self.scale)\n",
    "        if self.masked:\n",
    "            mask = (-1*torch.ones(3,3)*float('inf')).triu(1)\n",
    "            output += mask\n",
    "        output = output + q.size()[1]\n",
    "        outout = nn.Softmax(output, dim = 1)\n",
    "        output = torch.matmul(output, v)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, h):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_key = dim_key\n",
    "        self.dim_val = dim_val\n",
    "        \n",
    "        v_layers = []\n",
    "        k_layers = []\n",
    "        q_layers = []\n",
    "        attention_layers = []\n",
    "        for i in range(self.h):\n",
    "            q_layers.append(nn.Linear(in_featurues=dim_model, out_features=dim_key, bias=False))\n",
    "            k_layers.append(nn.Linear(in_featurues=dim_model, out_features=dim_key, bias=False))\n",
    "            v_layers.append(nn.Linear(in_featurues=dim_model, out_features=dim_val, bias=False))\n",
    "            attention_layers.append(ScaledDPAttention(dim_key, dim_val))\n",
    "        self.linear = nn.Linear(in_features = h*dim_val, out_features=dim_model, bias=False)\n",
    "    \n",
    "    def forward(Q, K, V):\n",
    "        outs = []\n",
    "        for i in range(self.h):\n",
    "            q = q_layers[i](Q)\n",
    "            k = k_layers[i](K)\n",
    "            v = v_layers[i](V)\n",
    "            o = attention_layers[i](q, k, v)\n",
    "            outs.append(o)\n",
    "        output = torch.cat(outs, dim=2) # check dimenson \n",
    "        return self.linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48378401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        self.n_dim = n_dim\n",
    "        self.get_pos = lambda pos : [pos/(10000**(2*(i//2)/n_dim)) for i in range(n_dim)]\n",
    "        \n",
    "    def forward(batch_size, embedding_length):\n",
    "        code = np.array([self.get_pos[i] for i in range(embedding_length)])\n",
    "        position_encoding = np.zeros((embedding_length, n_dim))\n",
    "        position_encoding[:, 0::2] = np.sin(code[:, 0::2])\n",
    "        position_encoding[:, 1::2] = np.cos(code[:, 1::2])\n",
    "        return np.tile(position_encoding, (batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfa708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, dim_hidden, h=8):\n",
    "        super().__init__()        \n",
    "        self.attention = MultiHeadAttention(dim_model, dim_key, dim_val, h)\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(in_feature=dim_model, out_feature=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_feawture=hidden_size, out_feature=dim_model)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "    \n",
    "    def forward(X):\n",
    "        A = self.attention(Q = X, K = X, V = X)\n",
    "        A = self.norm(A + X)\n",
    "        F = self.FFN(A)\n",
    "        return self.norm(F + A)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, dim_hidden, h=8):\n",
    "        super().__init__()\n",
    "        self.masked_attention = MultiHeadAttention(dim_model, dim_key, dim_val, h, masked=True)\n",
    "        self.attention = MultiHeadAttention(dim_model, dim_key, dim_val, h, masked=False)\n",
    "        self.FFN = nn.Sequential(\n",
    "            nn.Linear(in_feature=dim_model, out_feature=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_feawture=hidden_size, out_feature=dim_model)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_model)\n",
    "    \n",
    "    def forward(inputs): # (X, encoder_feature)\n",
    "        X, features = inputs\n",
    "        masked_A = self.masked_attention(Q = X, K = X, V = X)\n",
    "        masked_A = self.norm(masked_A + X)\n",
    "        \n",
    "        A = self.attention(Q = masked_A, K = features, V = features)\n",
    "        A = self.norm(A + masked_A)\n",
    "        \n",
    "        F = self.FFN(A)\n",
    "        F = self.norm(F + A)\n",
    "        return (F, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2147c317",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18928/16449830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mencoder_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdecoder_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_model, dim_key, dim_val, dim_hidden, N=6, h=8):\n",
    "        super().__init__()\n",
    "        encoder_layers = []\n",
    "        decoder_layers = []\n",
    "        \n",
    "        for i in range(N):\n",
    "            encoder_layers.append(EncoderLayer(dim_model, dim_key, dim_val, dim_hidden, h))\n",
    "            decoder_layers.append(DecoderLayer(dim_model, dim_key, dim_val, dim_hidden, h)) \n",
    "\n",
    "        self.Encoder = nn.Sequential(*encoder_layers)\n",
    "        self.Decoder = nn.Sequential(*decoder_layers)        \n",
    "        self.FC = nn.Linear() # weights? bias?\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(inputs, outputs):\n",
    "        features = self.Encoder(inputs)\n",
    "        d = self.Decoder((outputs, features))\n",
    "        return self.sm(self.FC(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31de2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
