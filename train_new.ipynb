{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import Transformer\n",
    "from TransformerDataLoader import *\n",
    "from helper import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor *(self.model_size ** (-0.5) *min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def load_model(resume_iters, dim_model, dim_hidden, dim_vocab, num_layers, num_heads):\n",
    "    # [TODO] pass in variables\n",
    "    print('Loading the trained models from step {}...'.format(resume_iters))\n",
    "    model_path = os.path.join('model/transformer_{}.pt'.format(resume_iters+1))\n",
    "\n",
    "    model = Transformer.TransformerModel(\n",
    "        dim_model, dim_hidden, dim_vocab, N=num_layers, h=num_heads)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def translate_sentence(sentence, src_field, trg_field, model, max_len=2000, logging=True):\n",
    "    model.eval()  # change into the evaluation mode\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # append <sos> token at the beginning and <eos> token at the end\n",
    "    tokens = [special_symbols[2]] + tokens + [special_symbols[3]]\n",
    "    if logging:\n",
    "        print(f\"full source token: {tokens}\")\n",
    "\n",
    "    src_indexes = src_field.lookup_indices(tokens)\n",
    "    if logging:\n",
    "        print(f\"source sentence index: {src_indexes}\")\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src_padding_mask = Transformer.get_padding_mask(\n",
    "        src_tensor, PAD_IDX).to(device)\n",
    "    enc_mem = model.encode(src_tensor, src_padding_mask)\n",
    "    # Make sure you have only one <sos> token at first\n",
    "    trg_indexes = [BOS_IDX]\n",
    "    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len-1):\n",
    "            # generate our output\n",
    "            trg_padding_mask = Transformer.get_padding_mask(\n",
    "                trg_tensor, PAD_IDX).to(device)\n",
    "            peek_mask = Transformer.get_peek_mask(trg_tensor).to(device)\n",
    "            dec = model.decode(trg_tensor, enc_mem,\n",
    "                               src_padding_mask, trg_padding_mask, peek_mask)\n",
    "            output = model.generator(dec)\n",
    "\n",
    "            pred_token = output.argmax(-1)[:, -1].item()\n",
    "            # trg_indexes.append(pred_token)  # add to output statement\n",
    "            trg_tensor = torch.cat([trg_tensor, torch.ones(\n",
    "                1, 1).type_as(src_tensor.data).fill_(pred_token)], dim=1)\n",
    "            # The moment you meet <eos>, it ends\n",
    "            if pred_token == EOS_IDX:\n",
    "                break\n",
    "\n",
    "    #  Convert each output word index to an actual word\n",
    "    trg_tokens = trg_field.lookup_tokens(trg_indexes)\n",
    "    # Returns the output statement excluding the first <sos>\n",
    "    return trg_tokens[1:]\n",
    "\n",
    "\n",
    "def show_bleu(data, src_field, trg_field, model, device, max_len=50):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    desc = '    - calculating BLEU - '\n",
    "    for d in tqdm(data[:100], mininterval=2, desc=desc, leave=False):  # debug\n",
    "        src, trg = d  # strings not tensor\n",
    "        pred_trg = translate_sentence(\n",
    "            src, src_field, trg_field, model, max_len, logging=False)\n",
    "        # Remove the last <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "\n",
    "    bleu = bleu_score(pred_trgs, trgs, max_n=4,\n",
    "                      weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    individual_bleu1_score = bleu_score(\n",
    "        pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
    "    individual_bleu2_score = bleu_score(\n",
    "        pred_trgs, trgs, max_n=4, weights=[0, 1, 0, 0])\n",
    "    individual_bleu3_score = bleu_score(\n",
    "        pred_trgs, trgs, max_n=4, weights=[0, 0, 1, 0])\n",
    "    individual_bleu4_score = bleu_score(\n",
    "        pred_trgs, trgs, max_n=4, weights=[0, 0, 0, 1])\n",
    "\n",
    "    print(f'BLEU Score = {bleu*100:.2f}'\n",
    "          + f'| BLEU-1 = {individual_bleu1_score*100:.2f} | BLEU-2 = {individual_bleu2_score*100:.2f}'\n",
    "          + f'| BLEU-3 = {individual_bleu3_score*100:.2f} | BLEU-4 = {individual_bleu4_score*100:.2f}')\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def cal_loss(pred, gold, trg_pad_idx, smoothing=True):\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(-1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(trg_pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).mean()\n",
    "    else:\n",
    "        loss = F.cross_entropy(\n",
    "            pred, gold, ignore_index=trg_pad_idx, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, epoch_num,clip=1, log_iter=5000):\n",
    "    model.train()  # set as train model\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # iterate through whole data set\n",
    "    # [TODO]: Add progress bar\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.optimizer.zero_grad()\n",
    "\n",
    "        trg_in = trg[:, :-1].clone().to(device)\n",
    "        trg_y = trg[:, 1:].clone().to(device)\n",
    "        # exclude the end of the sentence\n",
    "        # start with the beginning of the sentence\n",
    "        src_padding_mask = Transformer.get_padding_mask(\n",
    "            src, PAD_IDX).to(device)\n",
    "        trg_padding_mask = Transformer.get_padding_mask(\n",
    "            trg_in, PAD_IDX).to(device)\n",
    "        peek_mask = Transformer.get_peek_mask(trg_in).to(device)\n",
    "        output = model(src, trg_in, src_padding_mask,\n",
    "                       trg_padding_mask, peek_mask)\n",
    "        # output: [batch size, trg_len - 1, output_dim]\n",
    "        # trg: [batch size, trg_len]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # Exclude index 0 (<sos>) of output word\n",
    "        trg_y = trg_y.contiguous().view(-1)\n",
    "        # calculate the loss\n",
    "        loss = cal_loss(output, trg_y, PAD_IDX, True)\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        # calculate total loss\n",
    "        epoch_loss += loss.item()\n",
    "        if (i+1) % log_iter == 0:\n",
    "            train_ppl = math.exp(min(loss, 100))\n",
    "            logging.info(\n",
    "                f\"epoch {epoch_num} | bacth: {i+1}/{len(iterator)} | train_loss: {loss:.3f} | train_ppl: {train_ppl}\")\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()  # change to evaluation mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Checking the entire evaluation data\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            trg_in = trg[:, :-1].clone().to(device)\n",
    "            trg_y = trg[:, 1:].clone().to(device)\n",
    "\n",
    "            # exclude the end of the sentence\n",
    "            # start with the beginning of the sentence\n",
    "            src_padding_mask = Transformer.get_padding_mask(\n",
    "                src, PAD_IDX).to(device)\n",
    "            trg_padding_mask = Transformer.get_padding_mask(\n",
    "                trg_in, PAD_IDX).to(device)\n",
    "            peek_mask = Transformer.get_peek_mask(trg_in).to(device)\n",
    "            output = model(src, trg_in, src_padding_mask,\n",
    "                           trg_padding_mask, peek_mask)\n",
    "\n",
    "            # output: [batch size, trg_len - 1, output_dim]\n",
    "            # trg: [batch size, trg_len]\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            # Exclude index 0 (<sos>) of output word\n",
    "            trg_y = trg_y.contiguous().view(-1)\n",
    "\n",
    "            # output: [batch size * trg_len - 1, output_dim]\n",
    "            # trg: [batch size * trg len - 1]\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = cal_loss(output, trg_y, PAD_IDX, True)\n",
    "\n",
    "            # calculate total loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            break  # debug\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def train_model(model, train_iterator, valid_iterator, test_iterator, optimizer, n_epochs, clip, args):\n",
    "\n",
    "    # save the losses\n",
    "    log = []\n",
    "    # ignore the padding index\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    best_valid_loss = float('inf')\n",
    "    src_vocab = train_iterator.dataset.vocab['src']\n",
    "    trg_vocab = train_iterator.dataset.vocab['trg']\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        metrics = {}\n",
    "        start_time = time.time()  # record the start time\n",
    "\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
    "        train_ppl = math.exp(min(train_loss, 100))\n",
    "\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "        valid_ppl = math.exp(min(valid_loss, 100))\n",
    "\n",
    "        end_time = time.time()  # record the end time\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        test_loss = evaluate(model, test_iterator, criterion)\n",
    "        test_ppl = math.exp(min(test_loss, 100))\n",
    "\n",
    "        logging.info(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s |'\n",
    "                     + f' valid_loss: {valid_loss:.3f} valid_ppl: {valid_ppl} |'\n",
    "                     + f' test_loss: {test_loss:.3f}  test_ppl: {test_ppl}')\n",
    "\n",
    "        checkpoint = {'epoch': epoch, 'model': model.state_dict()}\n",
    "\n",
    "        save_path = os.path.join(\n",
    "            args.log_dir, 'transformer_epoch_{}.ckpt'.format(epoch))\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            model_name = os.path.join(args.log_dir, 'transformer_best.ckpt')\n",
    "            torch.save(checkpoint, model_name)\n",
    "            logging.info(f\"Updated best model to: {model_name}\")\n",
    "\n",
    "        metrics['train_loss'] = train_loss\n",
    "        metrics['train_PPL'] = train_ppl\n",
    "\n",
    "        metrics['valid_loss'] = valid_loss\n",
    "        metrics['valid_PPL'] = valid_ppl\n",
    "        # losses['valid_bleu_{}'.format(epoch)] = show_bleu(\n",
    "        #     valid_iterator.dataset.data, src_vocab, trg_vocab, model, device)\n",
    "\n",
    "        metrics['test_loss'] = test_loss\n",
    "        metrics['test_PPL'] = test_ppl\n",
    "        # losses['test_bleu_{}'.format(epoch)] = show_bleu(\n",
    "        #     test_iterator.dataset.data, src_vocab, trg_vocab, model, device)\n",
    "\n",
    "        log += [metrics]\n",
    "        # [TODO]: change saving structure\n",
    "        # dump into json file\n",
    "        log_name = os.path.join(args.log_dir, 'train.log.json')\n",
    "        with open(log_name, 'w') as f:\n",
    "            json.dump(log, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_path\", type=str, default = \"wmt17_en_de_processed.pkl\")\n",
    "    parser.add_argument(\"--log_dir\", type=str, default = \"\\log\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=2)\n",
    "    parser.add_argument(\"--eval_only\", action='store_true')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.log_dir):\n",
    "        os.mkdir(args.log_dir)\n",
    "\n",
    "    logging_dir = os.path.join(args.log_dir, \"debug.log\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(logging_dir),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # config\n",
    "    dim_model = 128\n",
    "    dim_hidden = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    n_epochs = 10\n",
    "    warmup = 4000\n",
    "    clip = 1\n",
    "\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def initialize_weights(m):\n",
    "        if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "    if not args.eval_only:\n",
    "        logging.info(\"Running Train\")\n",
    "        logging.info(f\"Loading Data: {args.data_path}\")\n",
    "        train_iterator = TransformerDataLoader(\n",
    "            args.data_path, 'train', batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "        valid_iterator = TransformerDataLoader(\n",
    "            args.data_path, 'valid', batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "        test_iterator = TransformerDataLoader(\n",
    "            args.data_path, 'test', batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "\n",
    "        logging.info(f\"training dataset: {len(train_iterator.dataset.data)} \\n\"\n",
    "                     + f\"validation dataset: {len(valid_iterator.dataset.data)} \\n\"\n",
    "                     + f\"testing dataset: {len(test_iterator.dataset.data)} \\n\")\n",
    "\n",
    "        logging.info(f\"src_vocab: {len(train_iterator.dataset.vocab['src'])} \\n\"\n",
    "                     + f\"trg_vocab: {len(train_iterator.dataset.vocab['trg'])} \\n\")\n",
    "        logging.info(f\"Running on: {device}\")\n",
    "        model = Transformer.TransformerModel(\n",
    "            dim_model, dim_hidden, len(train_iterator.dataset.vocab['trg']), N=num_layers, h=num_heads).to(device)\n",
    "\n",
    "        logging.info(model)\n",
    "        logging.info(\n",
    "            f'Created model: The model has {count_parameters(model)} trainable parameters')\n",
    "        model.apply(initialize_weights)\n",
    "        # Adam optimizer with lr scheduling\n",
    "        optimizer = NoamOpt(dim_model, 1, warmup,\n",
    "                            torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9))\n",
    "        train_model(model, train_iterator, valid_iterator,\n",
    "                    test_iterator, optimizer, n_epochs, clip, args)\n",
    "    else:\n",
    "        # [TODO]: eval only\n",
    "        pass\n",
    "\n",
    "    # [TODO]: add args\n",
    "    # [TODO]: Add logging"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] [--log_dir LOG_DIR]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS] [--eval_only]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/guoyifan/Library/Jupyter/runtime/kernel-95c86ea3-ac04-43ae-be99-8bd5030636bc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}